.. _data_extraction:

Data Extraction
===============

This section explains how to write the rules for extracting data
out of a document. We'll scrape the following fictional HTML page for the movie
"The Shining" in our examples:

.. literalinclude:: ../tests/shining.html
   :language: html

Assuming this file is saved as :file:`shining.html`, let's get its content:

.. code-block:: python

   >>> document = open('shining.html').read()

Although applications will mostly use the higher-level ``scrape`` function,
it's the ``extract`` function that handles the data extraction. So we'll
start by explaining how that works. First, let's import the functions we will
use:

>>> from piculet import build_tree, extract

The :func:`build_tree` function converts an XML document into a tree
and returns its root node: [#xhtml]_

>>> root = build_tree(document)

The :func:`extract` function takes the root node of an XML tree and
the rules for extraction. It applies the rules to the root node and returns
a mapping where each item is generated by one of the rules.

Each rule is itself a mapping. The ``key`` specifies the key for the item
in the output mapping and the ``value`` specifies how to extract the data
to set as the value for the item. Typically, a value specifier consists
of a path query and a reducing function. The query gets applied to the root
and produces a list of strings. Then the reducing function converts this list
into a single string. [#reducing]_

.. note::

   Piculet uses the `ElementTree`_ module for building and querying
   XML trees. Therefore, the XPath queries are limited by
   `what ElementTree supports`_ (plus the ``text()`` and ``@attr``
   clauses which are added by Piculet). However, Piculet will make use of
   the `lxml`_ package if it's installed and in that case,
   a `much wider range of XPath constructs`_ can be used.

.. _ElementTree: https://docs.python.org/3/library/xml.etree.elementtree.html
.. _what ElementTree supports: https://docs.python.org/3/library/xml.etree.elementtree.html#xpath-support
.. _lxml: http://lxml.de/
.. _much wider range of XPath constructs: http://lxml.de/xpathxslt.html#xpath

For example, to extract the title and year of the movie from the document,
we can write the following specification:

>>> items = [
...     {
...         "key": "title",
...         "value": {
...             "path": ".//title/text()",
...             "reduce": lambda xs: xs[0]
...         }
...     },
...     {
...         "key": "year",
...         "value": {
...             "path": ".//span[@class='year']/text()",
...             "reduce": lambda xs: xs[0]
...         }
...     }
... ]
>>> extract(root, items)
{'title': 'The Shining', 'year': '1980'}

For the ``title`` item the ``.//title/text()`` path generates the list
``['The Shining']`` and the reducing function ``lambda xs: xs[0]`` selects
the first element from that list.

If a path doesn't match any element in the tree, the item will be excluded
from the output and no error will be issued. Note that in the following
example, the "foo" key doesn't get included:

>>> items = [
...     {
...         "key": "title",
...         "value": {
...             "path": ".//title/text()",
...             "reduce": reducers.first
...          }
...     },
...     {
...         "key": "foo",
...         "value": {
...             "path": ".//foo/text()",
...             "reduce": reducers.first
...         }
...     }
... ]
{'title': 'The Shining'}

Reducing
--------

Piculet contains a few predefined reducing functions which can be accessed
through the ``reducers`` attribute::

>>> from piculet import reducers

The reducing function that returns the first element of a list
-as used in the examples above- is one of the predefined reducers:

.. code-block:: python

   {
       "path": ".//title/text()",
       "reduce": reducers.first
   }

Other common predefined reducing operations are joining and cleaning.
The ``join`` reducer joins all selected strings into one: [#join]_

>>> items = [
...     {
...         "key": "full_title",
...         "value": {
...             "path": ".//h1//text()",
...             "reduce": reducers.join
...         }
...     }
... ]
>>> extract(document, items)
{'full_title': 'The Shining (1980)'}

If you want to get rid of the extra whitespace, you can use the ``clean``
reducer. After joining the strings, this will remove leading and trailing
whitespace and replace multiple whitespace characters with a single space:

>>> items = [
...     {
...         "key": "review",
...         "value": {
...             "path": ".//div[@class='review']//text()",
...             "reduce": reducers.clean
...         }
...     }
... ]
{'review': 'Fantastic movie. Definitely recommended.'}

In this example, the ``join`` reducer would have produced the value
``'\n            Fantastic movie.\n            Definitely recommended.\n        '``

The predefined reducers can also be accessed by name using the "reducer"
key instead of "reduce" in the specification. So the title rule can also
be written as:

.. code-block:: python

   {
       "path": ".//title/text()",
       "reducer": "first"
   }

This might be useful if you would like to keep the rule specification
in an external file, such as a ``.json`` file.

If both a "reduce" and a "reducer" key are given, the "reduce" callable
will be used and the "reducer" key will be ignored.

As explained above, if a path query doesn't match any element, the item
gets automatically excluded. That means, Piculet doesn't try to apply
the reducing function on the result of the path query if it's an empty list.
Therefore, reducing functions can safely assume that the path result is
a non-empty list.

Transforming
------------

After the reduction operation, you can also apply a transformation
to the resulting string. A transformation function must take a string
as parameter and can return any value of any type. For example,
to get the year of the movie as an integer:

>>> items = [
...     {
...         "key": "year",
...         "value": {
...             "path": ".//span[@class='year']/text()",
...             "reduce": reducers.first,
...             "transform": int
...         }
...     }
... ]
>>> extract(root, items)
{'year': 1980}

Multi-valued Items
------------------

Data with multiple values can be created by using a ``foreach`` key
in the value specifier. This is a path expression to select elements
from the tree. [#multivalued]_ The path and reducing function will be applied
as before to each selected element and the obtained values will be the members
of the resulting list. For example, to get the genres of the movie,
we can write:

>>> items = [
...     {
...         "key": "genres",
...         "value": {
...             "foreach": ".//ul[@class='genres']/li",
...             "path": "./text()",
...             "reduce": reducers.first
...         }
...     }
... ]
>>> extract(document, items)
{'genres': ['Horror', 'Drama']}

If the ``foreach`` key doesn't match any element the item will be excluded
from the result:

>>> items = [
...     {
...         "key": "foos",
...         "value": {
...             "foreach": ".//ul[@class='foos']/li",
...             "path": "./text()",
...             "reduce": reducers.first
...         }
...     }
... ]
>>> extract(document, items)
{}

If a transformation is specified, it will be applied to every element
in the list:

>>> items = [
...     {
...         "key": "genres",
...         "value": {
...             "foreach": ".//ul[@class='genres']/li",
...             "path": "./text()",
...             "reduce": reducers.first,
...             "transform": lambda x: x.lower()
...         }
...     }
... ]
>>> extract(document, items)
{'genres': ['horror', 'drama']}

Subrules
--------

Nested structures can be created by writing subrules as value specifiers. If
the value specifier is a mapping that contains an ``items`` key, then this will
be interpreted as a subrule and the generated mapping will be the value for
the key.

>>> items = [
...     {
...         "key": "title",
...         "value": {
...             "path": ".//title/text()",
...             "reducer": "first"
...         }
...     },
...     {
...         "key": "director",
...         "value": {
...             "items": [
...                 {
...                     "key": "name",
...                     "value": {
...                         "path": ".//div[@class='director']//a/text()",
...                         "reducer": "first"
...                     }
...                 },
...                 {
...                     "key": "link",
...                     "value": {
...                         "path": ".//div[@class='director']//a/@href",
...                         "reducer": "first"
...                     }
...                 }
...             ]
...         }
...     }
... ]
>>> extract(document, items)
{'title': 'The Shining', 'director': {'name': 'Stanley Kubrick', 'link': '/people/1'}}

Subrules can also be combined with lists:

>>> items = [
...     {
...         "key": "cast",
...         "value": {
...             "foreach": ".//table[@class='cast']/tr",
...             "items": [
...                 {
...                     "key": "name",
...                     "value": {
...                         "path": "./td[1]/a/text()",
...                         "reducer": "first"
...                     }
...                 },
...                 {
...                     "key": "link",
...                     "value": {
...                         "path": "./td[1]/a/@href",
...                         "reducer": "first"
...                     }
...                 },
...                 {
...                     "key": "character",
...                     "value": {
...                         "path": "./td[2]/text()",
...                         "reducer": "first"
...                     }
...                 }
...             ]
...         }
...     }
... ]
>>> extract(document, items)
{'cast': [{'name': 'Jack Nicholson', 'link': '/people/2', 'character': 'Jack Torrance'}, {'name': 'Shelley Duvall', 'link': '/people/3', 'character': 'Wendy Torrance'}]}

Generating Keys from Content
----------------------------

You can generate items in the result mapping where the key value also comes
from the content. Consider for example how you would get the runtime and
the language from the sample document. One way of achieving this would be
to select the ``div`` elements with the ``info`` class and using the ``h3``
text as the key. Such sections in the document can be specified using
``foreach`` specifications in the keys. This will cause a new item to be
generated for each of the selected elements. To get the key value, we can use
paths and reducers that will be applied to each selected element. In the below
example, the ``normalize`` reducer joins the strings, converts it to lowercase,
replaces spaces with underscores and strips other non-alphanumeric characters:

>>> items = [
...     {
...         "foreach": ".//div[@class='info']",
...         "key": {
...             "path": "./h3/text()",
...             "reducer": "normalize"
...         },
...         "value": {
...             "path": "./div//text()",
...             "reducer": "clean"
...         }
...     }
... ]
>>> extract(document, items)
{'language': 'English', 'runtime': '144 minutes'}

You could also give a string instead of a path and reducer for the key.
In this case, the elements would still be traversed; only the last one would
set the final value for the item. This could make sense if you are sure
that there is only one element that matches the ``foreach`` path of the key.

.. [#xhtml]

   Note that our example document is well-formed XML.

.. [#reducing]

   This means that the query has to end with either ``text()`` or some
   attribute value as in ``@attr``. And the reducing function should be
   implemented so that it takes a list of strings and returns a string.

.. [#join]

   It's equivalent to:

   .. code-block:: python

      lambda xs: ''.join(xs)

.. [#multivalued]

   This implies that the ``foreach`` query should **not** end in ``text()``
   or ``@attr``.
